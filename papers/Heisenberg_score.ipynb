{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T06:33:38.909664Z",
     "start_time": "2021-04-07T06:33:38.905912Z"
    }
   },
   "outputs": [],
   "source": [
    "## This file is part of Jax Geometry\n",
    "#\n",
    "# Copyright (C) 2021, Stefan Sommer (sommer@di.ku.dk)\n",
    "# https://bitbucket.org/stefansommer/jaxgeometry\n",
    "#\n",
    "# Jax Geometry is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# Jax Geometry is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with Jax Geometry. If not, see <http://www.gnu.org/licenses/>.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score matching on the Heisenberg group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxgeometry.manifolds.Heisenberg import *\n",
    "M = Heisenberg()\n",
    "print(M)\n",
    "from jaxgeometry.plotting import *\n",
    "from IPython.display import clear_output, Image\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "global key\n",
    "seed = 42434154\n",
    "#import os; seed = int(os.urandom(5).hex(), 16)\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# element, tangent vector and covector\n",
    "X = M.coords([0.,0.,0.])\n",
    "\n",
    "## sub-Riemannian structure\n",
    "from jaxgeometry.sR import metric\n",
    "metric.initialize(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T06:39:02.226293Z",
     "start_time": "2021-04-07T06:38:08.643994Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from jaxgeometry.stochastics import Brownian_sR\n",
    "Brownian_sR.initialize(M)\n",
    "\n",
    "_dts = dts(n_steps=1000)\n",
    "(ts,xs,charts) = M.Brownian_sR(X,_dts,dWs(M.sR_dim,_dts))\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plot_path(zip(xs,charts))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product sde\n",
    "from jaxgeometry.stochastics import product_sde\n",
    "from jaxgeometry.stochastics.product_sde import tile\n",
    "(product,sde_product,chart_update_product) = product_sde.initialize(M,M.sde_Brownian_sR,M.chart_update_Brownian_sR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinate representation from point in embedding space, i.e. recover x from F(x)\n",
    "def get_coords(x):\n",
    "    chart = M.chart()\n",
    "    return (x,chart)\n",
    "\n",
    "# map embedding space tangent vector to the tangent bundle\n",
    "def to_TM(Fx,v):\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_field(X,xys,zs,view=(35,45)):\n",
    "    # Create a meshgrid for the 3D space\n",
    "    xm, ym, zm = np.meshgrid(xys,xys,zs)\n",
    "\n",
    "    # Define the vector field (u, v, w are the components of the field)\n",
    "    #s = net.apply(params, jnp.stack((xm.flatten(),ym.flatten(),zm.flatten())).T)\n",
    "    s = jnp.squeeze(jax.vmap(X)(jnp.stack((xm.flatten(),ym.flatten(),zm.flatten())).T))\n",
    "    # s = jax.vmap(lambda x: M.D((x,0)))(jnp.stack((xm.flatten(),ym.flatten(),zm.flatten())).T)[:,:,0]\n",
    "    u = s[:,0].reshape(xm.shape); v = s[:,1].reshape(ym.shape); w = s[:,2].reshape(zm.shape);\n",
    "\n",
    "    # Calculate the magnitude of vectors\n",
    "    magnitude = np.linalg.norm(s,axis=1)\n",
    "    magnitude = magnitude * 2\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.quiver(xm, ym, zm, u, v, w, color=plt.cm.jet(magnitude), length=1., linewidth=1.0, normalize=False)\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n",
    "    ax.set_xlim([jnp.min(xys), jnp.max(xys)]); ax.set_ylim([jnp.min(xys),jnp.max(xys)]); \n",
    "    ax.set_zlim([-1.,1.]); # ax.set_zlim([jnp.min(zs),jnp.max(zs)])\n",
    "    ax.view_init(*view)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "sweep_configuration = {\n",
    " \"name\": \"sR-Heisenberg\",\n",
    " \"metric\": {\"name\": \"loss\", \"goal\": \"minimize\"},\n",
    " \"method\": \"grid\",\n",
    " \"parameters\": {\n",
    "    \"learning_rate\": {\"values\": [.01]}, # Learning rate for optimizer\n",
    "    \"epochs\": {\"values\": [2500]}, # Number of passes over the dataset\n",
    "    \"batches_per_epoch\": {\"values\": [2**3]}, # batches per epoch\n",
    "    \"batch_size\": {\"values\": [2**6]}, # usually multiple of samples_per_x0\n",
    "    \"nodes\": {\"values\": [(10,15,15),(15,15,15),(10,10,15),(10,10,20)]}, # nodes per layer\n",
    "    \"activation\": {\"values\": ['relu']}, # nodes per layer\n",
    "    \"T\": {\"values\": [1.]}, # max diffusion time\n",
    "    \"n_steps\": {\"values\": [1000]}, # time discretization\n",
    "    },\n",
    "}\n",
    "\n",
    "def train(run=False,wandbmode='online'):\n",
    "    if run is False:\n",
    "        run = wandb.init(mode=wandbmode)\n",
    "    print(wandb.config)\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "    epochs = wandb.config.epochs\n",
    "    batch_size = wandb.config.batch_size\n",
    "    batches_per_epoch = wandb.config.batches_per_epoch\n",
    "    nodes = wandb.config.nodes\n",
    "    activation = wandb.config.activation\n",
    "    T = wandb.config.T\n",
    "    n_steps = wandb.config.n_steps\n",
    "    _dts = dts(T=T,n_steps=n_steps)\n",
    "    dt = _dts[0]\n",
    "    _ts = jnp.cumsum(_dts)\n",
    "    \n",
    "    x = M.coords([0.,0.,0.])\n",
    "    global x0s\n",
    "    x0s = (jnp.tile(x[0],(batch_size,1)),jnp.tile(x[1],(batch_size,1)))\n",
    "\n",
    "    def generator():\n",
    "        while True:\n",
    "            \"\"\"Generates batches of samples.\"\"\"\n",
    "            N = batch_size\n",
    "            _dWs = dWs(N*M.sR_dim,_dts).reshape(-1,N,M.sR_dim)\n",
    "            global x0s\n",
    "            (ts,xss,chartss,*_) = product(x0s,_dts,_dWs,jnp.repeat(1.,N))\n",
    "            #x0s = (xss[-1],chartss[-1])\n",
    "            yield jnp.concatenate((jax.vmap(lambda x,chart: M.F((x,chart)))(xss,chartss),_dWs),-1)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(generator,output_types=tf.float32,\n",
    "                                        output_shapes=([n_steps,batch_size,M.dim+M.sR_dim]))\n",
    "\n",
    "    print(ds)\n",
    "    print(ds.element_spec)\n",
    "    ds = iter(tfds.as_numpy(ds))\n",
    "    ## plot sample\n",
    "    #Fxs = next(ds)\n",
    "    ## plot\n",
    "    #newfig()\n",
    "    #M.plot()\n",
    "    #for i in range(batch_size):\n",
    "    #     M.plot_path(Fxs[:,i,0:M.dim])\n",
    "    #     M.plotx(Fxs[-1,i,0:M.dim],color='r')\n",
    "    #plt.show()\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            \"\"\"Create model.\"\"\"\n",
    "            model = nn.Sequential(\n",
    "                sum([(nn.Dense(node),getattr(jax.nn,activation)) for node in nodes],())+(nn.Dense(M.sR_dim),)\n",
    "            )\n",
    "            t = x[...,0]; Fx = x[...,1:]\n",
    "            return model(x)/t\n",
    "\n",
    "    # initialize net and parameters\n",
    "    s1 = Net() # score\n",
    "    s2 = Net() # diagonal of gradient of score\n",
    "    s1_div = Net() # score with divergence loss\n",
    "    global key\n",
    "    key,subkey = jax.random.split(key)\n",
    "    params_s1 = s1.init(subkey, jnp.zeros((1+M.dim)))\n",
    "    params_s2 = s2.init(subkey, jnp.zeros((1+M.dim)))\n",
    "    params_s1_div = s1_div.init(subkey, jnp.zeros((1+M.dim)))\n",
    "\n",
    "    def loss_s1(params, data):\n",
    "        \"\"\" compute denoising loss \"\"\"\n",
    "        def f(t,xnoise): \n",
    "            Fx = xnoise[0:M.dim]\n",
    "            delta = sigma2 = dt; sigma = jnp.sqrt(sigma2) \n",
    "            noise = xnoise[M.dim:]; z = noise/sigma\n",
    "            _s1 = s1.apply(params,jnp.hstack((t,Fx)))\n",
    "            return jnp.sum(jnp.dot(_s1,delta*_s1+2*noise)) # (5.1) in Grong, Habermann, Sommer 2024\n",
    "        #    return jnp.sum(jnp.square(-z/sigma-_s1))\n",
    "        v = jnp.mean(jax.vmap(\n",
    "                        jax.vmap(\n",
    "                            f,\n",
    "                        (0,0)),\n",
    "                    (None,1))(_ts,data))\n",
    "        return v\n",
    "    def loss_s1_Heisenberg(params, data):\n",
    "        \"\"\" compute denoising loss \"\"\"\n",
    "        def f(t,Fxnoise,tildeFx): \n",
    "            Fx = Fxnoise[0:M.dim]\n",
    "            delta = sigma2 = dt; sigma = jnp.sqrt(sigma2) \n",
    "            _s1 = s1.apply(params,jnp.hstack((t,Fx)))\n",
    "            hatS = -jnp.array(\n",
    "                [(Fx[0]-tildeFx[0])-jnp.pi*(Fx[1]-tildeFx[1])*jnp.sign(Fx[2]-tildeFx[2]-(tildeFx[0]*Fx[1]-Fx[0]*tildeFx[1])/2)/2,\n",
    "                 (Fx[1]-tildeFx[1])+jnp.pi*(Fx[0]-tildeFx[0])*jnp.sign(Fx[2]-tildeFx[2]-(tildeFx[0]*Fx[1]-Fx[0]*tildeFx[1])/2)/2]\n",
    "                 )\n",
    "            return jnp.sum(jnp.dot(_s1,delta*_s1-2*hatS)) # \n",
    "            # left to implement (5.5) in Grong, Habermann, Sommer 2024\n",
    "            #hatS = lambda q: -jnp.array(\n",
    "            #    [q[0]-jnp.pi*jnp.sign(q[2])*q[1],\n",
    "            #     q[1]+jnp.pi*jnp.sign(q[2])*q[0]]\n",
    "            #)\n",
    "            #return jnp.sum(jnp.dot(_s1,delta*_s1-2*hatS(Delta_hatX))) # (5.5) in Grong, Habermann, Sommer 2024\n",
    "        tildeFx = jnp.concatenate((jnp.expand_dims(x0s[0],0),data[:-1,:,0:M.dim]),axis=0)\n",
    "        v = jnp.mean(jax.vmap(\n",
    "                        jax.vmap(\n",
    "                            f,\n",
    "                        (0,0,0)),\n",
    "                    (None,1,1))(_ts,data,tildeFx))\n",
    "        return v\n",
    "    def loss_s2(params, data):\n",
    "        \"\"\" compute loss.\"\"\"\n",
    "        def f(t,xnoise): \n",
    "            Fx = xnoise[0:M.dim]\n",
    "            sigma2 = dt; sigma = jnp.sqrt(sigma2) \n",
    "            noise = xnoise[M.dim:]; z = noise/sigma\n",
    "            _s1 = s1.apply(params,jnp.hstack((t,Fx)))\n",
    "            _s2 = s2.apply(params,jnp.hstack((t,Fx)))\n",
    "            return jnp.sum(jnp.square(_s2+jnp.square(_s1)+(jnp.ones(M.sR_dim)-jnp.square(z))/sigma2))\n",
    "        v = jnp.mean(jax.vmap(\n",
    "                        jax.vmap(\n",
    "                            f,\n",
    "                        (0,0)),\n",
    "                    (None,1))(_ts,data))\n",
    "        return v\n",
    "    def loss_s1_div(params, data):\n",
    "        \"\"\" compute loss using divergence \"\"\"\n",
    "        def f(t,xnoise): \n",
    "            Fx = xnoise[0:M.dim]\n",
    "            x,chart = get_coords(Fx)\n",
    "            _s1 = s1_div.apply(params,jnp.hstack((t,Fx)))\n",
    "            norm2 = jnp.sum(jnp.square(_s1))\n",
    "            div = M.div((x,chart),lambda x: s1_div.apply(params,jnp.hstack((t,M.F(x)))))\n",
    "            return norm2+2*div # (4.3) in Grong, Habermann, Sommer 2024\n",
    "        v = jnp.mean(jax.vmap(\n",
    "                        jax.vmap(\n",
    "                            f,\n",
    "                        (0,0)),\n",
    "                    (None,1))(_ts,data))\n",
    "        return v\n",
    "\n",
    "    # Initialize solver.\n",
    "    opt = optax.adam(learning_rate)\n",
    "    opt_state_s1 = opt.init(params_s1)\n",
    "    opt_state_s1_div = opt.init(params_s1_div)\n",
    "    loss_grad_s1 = jax.jit(jax.value_and_grad(loss_s1,argnums=0))\n",
    "    loss_grad_s1_Heisenberg = jax.jit(jax.value_and_grad(loss_s1_Heisenberg,argnums=0))\n",
    "    loss_grad_s1_div = jax.jit(jax.value_and_grad(loss_s1_div,argnums=0))\n",
    "\n",
    "    # run training loop, s1\n",
    "    print(\"Training s1\")\n",
    "    for i in range(epochs*batches_per_epoch):\n",
    "      #loss_val, grads = loss_grad_s1(params_s1, next(ds))\n",
    "      loss_val, grads = loss_grad_s1(params_s1, next(ds))\n",
    "      updates, opt_state_s1 = opt.update(grads, opt_state_s1)\n",
    "      params_s1 = optax.apply_updates(params_s1, updates)\n",
    "      if i % 10 == 0:\n",
    "        wandb.log({'loss': loss_val, 'epoch': i//batches_per_epoch})\n",
    "      if i % batches_per_epoch == 0:\n",
    "        print(f\"[Step {i}], epoch {i//batches_per_epoch}, training loss: {loss_val:.3e}.\")\n",
    "\n",
    "    ## run training loop, s1 divergence loss\n",
    "    #print(\"Training s1_div\")\n",
    "    #for i in range(epochs*batches_per_epoch):\n",
    "    #  loss_val, grads = loss_grad_s1_div(params_s1_div, next(ds))\n",
    "    #  updates, opt_state_s1_div = opt.update(grads, opt_state_s1_div)\n",
    "    #  params_s1_div = optax.apply_updates(params_s1_div, updates)\n",
    "    #  if i % 10 == 0:\n",
    "    #    wandb.log({'loss': loss_val, 'epoch': i//batches_per_epoch})\n",
    "    #  if i % batches_per_epoch == 0:\n",
    "    #    print(f\"[Step {i}], epoch {i//batches_per_epoch}, training loss: {loss_val:.3e}.\")\n",
    "\n",
    "    #return (params_s1, s1), (params_s2, s2), (params_s1_div, s1_div)\n",
    "    #return (params_s1, s1), (params_s1_div, s1_div)\n",
    "    return (params_s1, s1),\n",
    "\n",
    "## run sweep\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"sR\")\n",
    "#wandb.agent(sweep_id, function=train)\n",
    "config = {'learning_rate': 1e-2, 'epochs': 2500, 'batches_per_epoch': 2**3, 'batch_size': 2**6, 'nodes': (15,15,15,), 'activation': 'relu', 'T': 1., 'n_steps': 1000}\n",
    "wandbmode=\"disabled\"\n",
    "with wandb.init(project='sR',config=config,mode=wandbmode) as run:\n",
    "    #(params_s1, s1), (params_s2, s2), (params_s1_div, s1_div) = train(run,wandbmode=wandbmode)\n",
    "    #(params_s1, s1), (params_s1_div, s1_div) = train(run,wandbmode=wandbmode)\n",
    "    (params_s1, s1), = train(run,wandbmode=wandbmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot xy-slices of the result for different t\n",
    "for t in jnp.linspace(.2,1,5+1):\n",
    "    print(\"t:\",t)\n",
    "    plot_field(lambda y: .2*t*jnp.dot(M.D(get_coords(y)),s1.apply(params_s1,jnp.hstack((t,y)))),jnp.arange(-1,1,.1),jnp.arange(.2,1,1),view=(90,0))\n",
    "    #plot_field(lambda y: .2*t*jnp.dot(M.D(get_coords(y)),s1_div.apply(params_s1_div,jnp.hstack((t,y)))),jnp.arange(-1,1,.1),jnp.arange(.2,1,1),view=(90,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot full 3D view\n",
    "# Generate a vector field\n",
    "X, Y, Z = np.mgrid[-1:1:15j, -1:1:15j, -1:1:15j]\n",
    "t = .2\n",
    "_s1 = jax.vmap(lambda x,y,z: t*jnp.dot(M.D(get_coords(jnp.hstack((x,y,z)))),s1.apply(params_s1,jnp.hstack((t,x,y,z)))),(0,0,0))(X.flatten(),Y.flatten(),Z.flatten()).reshape(X.shape+(1+M.sR_dim,))\n",
    "u = _s1[...,0]; v = _s1[...,1]; w = _s1[...,2]\n",
    "\n",
    "# Calculate the magnitude of vectors\n",
    "magnitude = np.linalg.norm(_s1,axis=3)\n",
    "#magnitude = magnitude / magnitude.max()\n",
    "magnitude = magnitude / 4\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(X, Y, Z, u, v, w, color=plt.cm.jet(magnitude).reshape((-1,4)), length=.1)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.view_init(35, 30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridges using learned score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" guide towards 0 in Heisenberg group \"\"\"\n",
    "x = M.coords([.5,0.,.8]) # starting point\n",
    "v = (jnp.zeros_like(X[0]),X[1]) # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guided process with explicit guide\n",
    "def guide(x,v,*_):\n",
    "    \"\"\" guided towards 0 in Heisenberg group \"\"\"\n",
    "    gamma = jnp.arctan2(x[0][1],x[0][0])\n",
    "    \n",
    "    f = lambda alpha: (8*jnp.sin(alpha[0]/2)**2*jnp.abs(x[0][2])-jnp.sum(x[0][0:2]**2)*(alpha[0]-jnp.sin(alpha[0])))**2\n",
    "    alpha = optimize.minimize(f,jnp.array([jnp.pi]),method='BFGS').x[0]\n",
    "    \n",
    "    r = jnp.linalg.norm(x[0][0:2])/(2*jnp.sin(alpha/2))\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    b = jax.lax.cond(jnp.abs(x[0][2])<epsilon,\n",
    "                     lambda _: \n",
    "                         jnp.array([-jnp.linalg.norm(x[0][0:2])*jnp.cos(gamma),\n",
    "                                    -jnp.linalg.norm(x[0][0:2])*jnp.sin(gamma)]),\n",
    "                     lambda _: \n",
    "                         jnp.array([-r*alpha*jnp.cos(gamma+jnp.sign(x[0][2])*alpha/2),\n",
    "                                    -r*alpha*jnp.sin(gamma+jnp.sign(x[0][2])*alpha/2)]),\n",
    "                     None)\n",
    "    return b\n",
    "\n",
    "# coordinate form\n",
    "from jaxgeometry.stochastics.guided_process import *\n",
    "\n",
    "(Brownian_sR_guided_explicit,_,*_) = get_guided(\n",
    "    M,M.sde_Brownian_sR,M.chart_update_Brownian_sR,guide,\n",
    "    lambda x,*_: jnp.linalg.cholesky(jnp.tensordot(M.D(x),M.D(x),(0,0))))\n",
    "\n",
    "# plot with zero nois##############i   ##dddadfadfiiiiiiiiiiiiie\n",
    "_dts = dts(n_steps=1000)\n",
    "(ts,xs,charts,log_likelihood,log_varphi) = Brownian_sR_guided_explicit(x,v,_dts,0*dWs(M.sR_dim,_dts),1.)\n",
    "xs_explicit,charts_explicit = xs,charts\n",
    "print(xs[-1])\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plot_path(zip(xs,charts))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(jnp.cumsum(_dts),jax.vmap(lambda x: jnp.linalg.norm(x[0:2]),0)(xs),'r')\n",
    "plt.plot(jnp.cumsum(_dts),jax.vmap(lambda x: jnp.linalg.norm(x[3]),0)(xs),'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bridge with score\n",
    "\n",
    "#guide = lambda x,t: jnp.dot(M.D(x).T,net.apply(params,jnp.hstack((t,x[0]))))\n",
    "guide = lambda x,t: s1.apply(params_s1,jnp.hstack((t,x[0])))\n",
    "\n",
    "sde = M.sde_Brownian_sR\n",
    "chart_update = M.chart_update_Brownian_sR\n",
    "def sde_guided(c,y):\n",
    "    t,x,chart,log_likelihood,log_varphi,T,v,*cy = c\n",
    "    xchart = (x,chart)\n",
    "    dt,dW = y\n",
    "    \n",
    "    (det,sto,X,*dcy) = sde((t,x,chart,*cy),y)\n",
    "    \n",
    "    h = jax.lax.cond(t<T-dt/2,\n",
    "                     lambda _: guide(xchart,T-t),\n",
    "                     lambda _: jnp.zeros_like(guide(xchart,T-t)),\n",
    "                     None)\n",
    "    \n",
    "    sto = jax.lax.cond(t < T-3*dt/2, # for Ito as well?\n",
    "                       lambda _: sto,\n",
    "                       lambda _: jnp.zeros_like(sto),\n",
    "                       None)\n",
    "\n",
    "    ### likelihood\n",
    "    log_likelihood = 0.\n",
    "\n",
    "    ## correction factor\n",
    "    log_varphi = 0.\n",
    "\n",
    "    return (det+jnp.dot(X,h),sto,X,log_likelihood,log_varphi,jnp.zeros_like(T),jnp.zeros_like(v),*dcy)\n",
    "\n",
    "def chart_update_guided(x,chart,log_likelihood,log_varphi,T,v,*ys):\n",
    "    if chart_update is None:\n",
    "        return (x,chart,log_likelihood,log_varphi,T,v,*ys)\n",
    "\n",
    "    (x_new, chart_new, *ys_new) = chart_update(x,chart,*ys)\n",
    "    v_new = M.update_coords((v,chart),chart_new)[0]\n",
    "    return (x_new,chart_new,log_likelihood,log_varphi,T,v_new,*ys_new)\n",
    "\n",
    "guided = lambda x,v,dts,dWs,*ys: integrate_sde(sde_guided,integrator_stratonovich,chart_update_guided,x[0],x[1],dts,dWs,0.,0.,jnp.sum(dts),M.update_coords(v,x[1])[0] if chart_update else v,*ys)[0:5]\n",
    "\n",
    "#(Brownian_sR_guided,sde_Brownian_sR_guided,*_) = get_guided(\n",
    "#    M,M.sde_Brownian_sR,M.chart_update_Brownian_sR,guide,\n",
    "#    lambda x: jnp.linalg.cholesky(jnp.tensordot(M.D(x),M.D(x),(0,0))))\n",
    "Brownian_sR_guided = guided\n",
    "\n",
    "T = .1\n",
    "_dts = dts(n_steps=1000,T=T)\n",
    "(ts,xs,charts,log_likelihood,log_varphi) = Brownian_sR_guided(x,v,_dts,dWs(M.sR_dim,_dts),1.)\n",
    "print(xs[-1])\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plot_path(zip(xs,charts))\n",
    "M.plot_path(zip(xs_explicit,charts_explicit),color='r')\n",
    "plt.savefig(f'guided_{T}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(jnp.cumsum(_dts),jax.vmap(lambda x: jnp.linalg.norm(x[0:2]),0)(xs),'r')\n",
    "plt.plot(jnp.cumsum(_dts),jax.vmap(lambda x: jnp.linalg.norm(x[2]),0)(xs),'b')\n",
    "plt.savefig(f'guided_norms_{T}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "N = 100\n",
    "for T in [.1,.2,.5,1.]:\n",
    "    _dts = dts(n_steps=1000,T=T)\n",
    "    xss = jax.vmap(lambda dWs: Brownian_sR_guided(x,v,_dts,dWs,1.)[1])(dWs(N*M.sR_dim,_dts).reshape((N,-1,M.sR_dim)))\n",
    "\n",
    "    xy = jax.vmap(jax.vmap(lambda x: jnp.linalg.norm(x[0:2]),0))(xss)\n",
    "    Z = jax.vmap(jax.vmap(lambda x: jnp.linalg.norm(x[2]),0))(xss)\n",
    "    mean_xy = jnp.median(xy,0)\n",
    "    mean_z = jnp.median(Z,0)\n",
    "    quartiles_xy = np.percentile(xy, [25, 50, 75], axis=0)\n",
    "    quartiles_z = np.percentile(Z, [25, 50, 75], axis=0)\n",
    "\n",
    "    # plot xy\n",
    "    plt.plot(jnp.cumsum(_dts),mean_xy,'r')\n",
    "    plt.fill_between(jnp.cumsum(_dts), quartiles_xy[0], quartiles_xy[2], alpha=0.3)\n",
    "    # plot z\n",
    "    plt.plot(jnp.cumsum(_dts),mean_z,'b')\n",
    "    plt.fill_between(jnp.cumsum(_dts), quartiles_z[0], quartiles_z[2], alpha=0.3)\n",
    "\n",
    "    plt.z_lim = (0.,1.3)\n",
    "    plt.savefig(f'guided_statistics_{T}.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c24204d473adc14cc533d6665816781de09c5dc8bb7e6bfcee384308cd893cca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
